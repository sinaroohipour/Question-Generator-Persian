{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2172031,"sourceType":"datasetVersion","datasetId":1303838},{"sourceId":3242192,"sourceType":"datasetVersion","datasetId":1965161}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q rouge rouge_score bert-score nltk evaluate hazm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install -U datasets ","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:26:19.018049Z","iopub.execute_input":"2024-10-16T12:26:19.018866Z","iopub.status.idle":"2024-10-16T12:26:33.314819Z","shell.execute_reply.started":"2024-10-16T12:26:19.018825Z","shell.execute_reply":"2024-10-16T12:26:33.313570Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from evaluate import load\nfrom prettytable import PrettyTable\nfrom rouge import Rouge\nimport json\nfrom datasets import Dataset \nfrom transformers import MT5Tokenizer, MT5ForConditionalGeneration, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler, MT5ForConditionalGeneration, MT5Tokenizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import DataLoader\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import AdamW\nimport evaluate\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\n# from rouge import Rouge","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:28:23.757489Z","iopub.execute_input":"2024-11-06T12:28:23.758194Z","iopub.status.idle":"2024-11-06T12:28:23.765595Z","shell.execute_reply.started":"2024-11-06T12:28:23.758155Z","shell.execute_reply":"2024-11-06T12:28:23.764723Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# import os\n# os.listdir('/kaggle/input/persianqa')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 1: Download the rouge.py file from GitHub and rename it\n# !wget https://raw.githubusercontent.com/Mofid-AI/persian-nlp-benchmark/main/rouge.py -O /kaggle/working/custom_rouge.py\n\n# # Step 2: Add the current directory to the system path and import the necessary class\n# import sys\n# sys.path.append('/kaggle/working')\n\n# from custom_rouge import RougeScorer\n\n# # # Step 3: Define reference and generated sentences\n# # ref = 'سینا امروز سخت کار میکند.'\n# # gen = 'سینا و نیما امروز کار نمیکنند.'\n\n# # # Initialize the RougeScorer for rouge1, rouge2, and rougeL\n# # rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n\n# # # Calculate the scores\n# # scores = rouge_scorer.score(ref, gen)\n\n# # # Print the results\n# # for rouge_type, score in scores.items():\n# #     print(f\"{rouge_type}: Precision: {score.precision}, Recall: {score.recall}, F-measure: {score.fmeasure}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the training data\nwith open('/kaggle/input/persianqa/pqa_train.json') as f:\n    persianqa_train = json.load(f)\n\n# Load the testing data\nwith open('/kaggle/input/persianqa/pqa_test.json') as f:\n    persianqa_test = json.load(f)\n    \nwith open(\"/kaggle/input/persianquad/train.json\") as f:\n    persianquad_train = json.load(f)\n    \nwith open(\"/kaggle/input/persianquad/test.json\") as f:\n    persianquad_test = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:32:26.748755Z","iopub.execute_input":"2024-11-06T12:32:26.749151Z","iopub.status.idle":"2024-11-06T12:32:27.059029Z","shell.execute_reply.started":"2024-11-06T12:32:26.749114Z","shell.execute_reply":"2024-11-06T12:32:27.058216Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Function to clean and keep only context and questions\ndef clean_dataset(data):\n    cleaned_data = []\n\n    for item in data['data']:\n        # Iterate through paragraphs\n        for paragraph in item['paragraphs']:\n            context = paragraph['context']\n            for qa in paragraph['qas']:\n                if not qa['is_impossible']:\n                    question = qa['question']\n                    cleaned_data.append({\n                        'context': context,\n                        'question': question\n                    })\n\n    return cleaned_data\n\n# Clean both train and test sets\ncleaned_persianqa_train = clean_dataset(persianqa_train)\ncleaned_persianqa_test = clean_dataset(persianqa_test)\ncleaned_persianquad_train = clean_dataset(persianquad_train)\ncleaned_persianquad_test = clean_dataset(persianquad_test)\n\n# Save the cleaned datasets\nwith open('cleaned_pqa_train.json', 'w') as file:\n    json.dump(cleaned_persianqa_train, file, ensure_ascii=False, indent=4)\n\nwith open('cleaned_pqa_test.json', 'w') as file:\n    json.dump(cleaned_persianqa_test, file, ensure_ascii=False, indent=4)\n    \nwith open('cleaned_pquad_train.json', 'w') as file:\n    json.dump(cleaned_persianquad_train, file, ensure_ascii=False, indent=4)\n\nwith open('cleaned_pquad_test.json', 'w') as file:\n    json.dump(cleaned_persianquad_test, file, ensure_ascii=False, indent=4)\n\nprint(\"Datasets cleaned and saved successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:32:31.919415Z","iopub.execute_input":"2024-11-06T12:32:31.919782Z","iopub.status.idle":"2024-11-06T12:32:32.515278Z","shell.execute_reply.started":"2024-11-06T12:32:31.919747Z","shell.execute_reply":"2024-11-06T12:32:32.514376Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Datasets cleaned and saved successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"with open('cleaned_pqa_train.json', 'r') as file:\n    qa_train = json.load(file)\n\nwith open('cleaned_pqa_test.json', 'r') as file:\n    qa_test = json.load(file)\n    \nwith open('cleaned_pquad_train.json', 'r') as file:\n    quad_train = json.load(file)\n    \nwith open('cleaned_pquad_test.json', 'r') as file:\n    quad_test = json.load(file)\n\ntrain = qa_train + quad_train\ntest = qa_test + quad_test\ntrain, val = train_test_split(train, test_size=0.15, random_state=42)\n\ntrain = train[0:64]\nval = val[0:32]\ntest = test[0:16]\n\n# Convert lists to datasets\ntrain_data = Dataset.from_list(train)\nval_data = Dataset.from_list(val)\ntest_data = Dataset.from_list(test)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:32:47.318223Z","iopub.execute_input":"2024-11-06T12:32:47.318630Z","iopub.status.idle":"2024-11-06T12:32:47.846783Z","shell.execute_reply.started":"2024-11-06T12:32:47.318591Z","shell.execute_reply":"2024-11-06T12:32:47.845840Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:32:54.567479Z","iopub.execute_input":"2024-11-06T12:32:54.567868Z","iopub.status.idle":"2024-11-06T12:32:54.575321Z","shell.execute_reply.started":"2024-11-06T12:32:54.567832Z","shell.execute_reply":"2024-11-06T12:32:54.574470Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question'],\n    num_rows: 64\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# max_len_qa = 0\n# min_len_qa = 12000\n# max_len_quad = 0\n# min_len_quad = 12000\n\n# for item in qa_train:\n#     if len(item['context'].split()) > max_len_qa:\n#         max_len_qa = len(item['context'].split())\n#         text = item['context']\n        \n# for item in qa_test:\n#     if len(item['context'].split()) > max_len_qa:\n#         max_len_qa = len(item['context'].split())\n#         text = item['context']\n        \n# for item in qa_train:\n#     if len(item['context'].split()) < min_len_qa:\n#         min_len_qa = len(item['context'].split())\n#         text = item['context']\n\n# for item in qa_test:\n#     if len(item['context'].split()) < min_len_qa:\n#         min_len_qa = len(item['context'].split())\n#         text = item['context']\n    \n# for item in quad_train:\n#     if len(item['context'].split()) > max_len_quad:\n#         max_len_quad = len(item['context'].split())\n#         text = item['context']\n        \n# for item in quad_test:\n#     if len(item['context'].split()) > max_len_quad:\n#         max_len_quad = len(item['context'].split())\n#         text = item['context']\n        \n# for item in quad_train:\n#     if len(item['context'].split()) < min_len_quad:\n#         min_len_quad = len(item['context'].split())\n#         text = item['context']\n\n# for item in quad_test:\n#     if len(item['context'].split()) < min_len_quad:\n#         min_len_quad = len(item['context'].split())\n#         text = item['context']\n# c=0\n# s=0\n# for item in quad_train:\n#     c+=1\n#     s+=len(item['context'].split())\n#     avg_quad_train = s/c\n    \n# c=0\n# s=0\n# for item in qa_train:\n#     c+=1\n#     s+=len(item['context'].split())\n#     avg_qa_train = s/c\n        \n# print(max_len_qa)\n# print(min_len_qa)\n# print(avg_qa_train)\n# print(max_len_quad)\n# print(min_len_quad)\n# print(avg_quad_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T19:07:25.970803Z","iopub.execute_input":"2024-10-15T19:07:25.971699Z","iopub.status.idle":"2024-10-15T19:07:27.105409Z","shell.execute_reply.started":"2024-10-15T19:07:25.971659Z","shell.execute_reply":"2024-10-15T19:07:27.104446Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\", legacy=False, clean_up_tokenization_spaces=True)\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-large\")","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:33:00.196633Z","iopub.execute_input":"2024-11-06T12:33:00.197453Z","iopub.status.idle":"2024-11-06T12:33:36.472596Z","shell.execute_reply.started":"2024-11-06T12:33:00.197412Z","shell.execute_reply":"2024-11-06T12:33:36.471720Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532b4736c295401e869519c782895771"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f45991755a3a404b86a8fbc67085f137"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"219b956a206a42ecb459b2c30886f932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11c9174f5a8743b8b64a5bc779151073"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10509c2f7b264160b1ea9c5ab8d90aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2fd73efe2c4138939877861a6ea7bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c3b6bf96ffd4e8aa713e675eb48d0de"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import Dataset as TDataset\n\nclass QuestionGenerationDataset(TDataset):\n    def __init__(self, data, tokenizer, max_input_length=360, max_output_length=64):\n        self.data_items = data\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_output_length = max_output_length\n\n    def __len__(self):\n        return len(self.data_items)\n\n    def __getitem__(self, idx):\n        context = self.data_items[idx]['context']\n        question = self.data_items[idx]['question']\n\n        inputs = self.tokenizer(\n            context + \" </s> سوال: \",\n            max_length=self.max_input_length,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        targets = self.tokenizer(\n            question,\n            max_length=self.max_output_length,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        input_ids = inputs.input_ids.squeeze(0)\n        attention_mask = inputs.attention_mask.squeeze(0)\n        labels = targets.input_ids.squeeze(0)\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:33:40.450672Z","iopub.execute_input":"2024-11-06T12:33:40.451056Z","iopub.status.idle":"2024-11-06T12:33:40.460781Z","shell.execute_reply.started":"2024-11-06T12:33:40.451016Z","shell.execute_reply":"2024-11-06T12:33:40.459591Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataset = QuestionGenerationDataset(train_data, tokenizer)\nval_dataset = QuestionGenerationDataset(val_data, tokenizer)\ntest_dataset = QuestionGenerationDataset(test_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:33:43.906029Z","iopub.execute_input":"2024-11-06T12:33:43.906420Z","iopub.status.idle":"2024-11-06T12:33:43.912698Z","shell.execute_reply.started":"2024-11-06T12:33:43.906381Z","shell.execute_reply":"2024-11-06T12:33:43.911550Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def save_model_safely(model):\n    output_dir = \"./results/checkpoint\"\n    os.makedirs(output_dir, exist_ok=True)\n    for param in model.parameters():\n        if not param.is_contiguous():\n            param.data = param.data.contiguous()\n\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:33:49.138943Z","iopub.execute_input":"2024-11-06T12:33:49.139789Z","iopub.status.idle":"2024-11-06T12:33:49.144772Z","shell.execute_reply.started":"2024-11-06T12:33:49.139747Z","shell.execute_reply":"2024-11-06T12:33:49.143875Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"%%capture\nimport torch.optim.lr_scheduler as lr_scheduler\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nnum_epochs = 3\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\nlogging_steps = 500\neval_steps = 1000\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T12:34:02.141814Z","iopub.execute_input":"2024-11-06T12:34:02.142167Z","iopub.status.idle":"2024-11-06T12:34:03.845136Z","shell.execute_reply.started":"2024-11-06T12:34:02.142134Z","shell.execute_reply":"2024-11-06T12:34:03.844381Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"loss_table = PrettyTable()\nloss_table.field_names = [\"Epoch\", \"Train Loss\", \"Val Loss\"]\nloss_log_file = \"training_loss_log.txt\"\n\ntrain_losses = []\nval_losses = []\nbest_eval_loss = float('inf')\nearly_stopping_counter = 0\nbest_epoch = 0\n\n# Training Loop\nloss_table = PrettyTable()\nloss_table.field_names = [\"Epoch\", \"Train Loss\", \"Val Loss\", \"Learning Rate\"]\nloss_log_file = \"training_loss_log.txt\"\n\ntrain_losses = []\nval_losses = []\nbest_eval_loss = float('inf')\nearly_stopping_counter = 0\nbest_epoch = 0\n\n# Training Loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    for step, batch in enumerate(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_train_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if (step + 1) % logging_steps == 0:\n            print(f\"Step {step+1}, Loss: {loss.item():.4f}\")\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    print(f\"Training loss: {avg_train_loss:.4f}\")\n\n    # Evaluation Loop\n    model.eval()\n    total_eval_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_eval_loss += loss.item()\n\n    avg_eval_loss = total_eval_loss / len(val_loader)\n    val_losses.append(avg_eval_loss)\n    print(f\"Validation loss: {avg_eval_loss:.4f}\")\n\n    # Get the learning rate from the optimizer (there could be multiple param groups)\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Log the epoch, train loss, val loss, and learning rate in PrettyTable\n    loss_table.add_row([epoch + 1, f\"{avg_train_loss:.4f}\", f\"{avg_eval_loss:.4f}\", f\"{current_lr:.6f}\"])\n\n    # Update learning rate based on validation loss\n    scheduler.step(avg_eval_loss)\n\n    if avg_eval_loss < best_eval_loss:\n        best_eval_loss = avg_eval_loss\n        early_stopping_counter = 0\n        best_epoch = epoch\n        save_model_safely(model)\n\n# Save the PrettyTable to the log file at the end of training\nwith open(loss_log_file, \"a\") as f:\n    f.write(str(loss_table) + \"\\n\")","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Plot Training and Validation Loss\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss', marker='o')\nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', marker='o')\nplt.title('Training and Validation Loss over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nloss_plot_file = \"training_val_loss_plot.png\"\nplt.savefig(loss_plot_file)\n\nplt.show()\nprint(f\"Training and validation loss diagram saved to {loss_plot_file}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:37:14.950188Z","iopub.execute_input":"2024-10-16T12:37:14.950536Z","iopub.status.idle":"2024-10-16T12:37:15.502423Z","shell.execute_reply.started":"2024-10-16T12:37:14.950501Z","shell.execute_reply":"2024-10-16T12:37:15.501488Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\nfrom bert_score import score as bert_score\nimport evaluate\nimport sys\n\n# Adding the path to custom_rouge.py\nsys.path.append('/kaggle/working')\n\n# Import the custom RougeScorer from the renamed file\nfrom custom_rouge import RougeScorer\n\n# Load other metrics\nrouge_metric = evaluate.load(\"rouge\")\nbleu_metric = evaluate.load(\"bleu\")\n\nnltk_smooth = SmoothingFunction().method4\n\ndef compute_metrics(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # ROUGE\n    rouge_scorer_obj = RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n    rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n    for pred, label in zip(preds, labels):\n        scores = rouge_scorer_obj.score(label, pred)\n        for key in rouge_scores:\n            rouge_scores[key] += scores[key].fmeasure\n    rouge_scores = {key: value / len(preds) for key, value in rouge_scores.items()}\n\n    # BLEU\n    bleu_scores = []\n    for pred, label in zip(preds, labels):\n        reference = [label.split()]  \n        candidate = pred.split()  \n        bleu = sentence_bleu(reference, candidate, smoothing_function=nltk_smooth)\n        bleu_scores.append(bleu)\n    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n\n    # METEOR\n    meteor_scores = []\n    for pred, label in zip(preds, labels):\n        reference_tokens = label.split()  # Tokenize reference\n        pred_tokens = pred.split()  # Tokenize prediction\n        meteor = meteor_score([reference_tokens], pred_tokens)  # Pass tokenized inputs\n        meteor_scores.append(meteor)\n        \n    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n    \n    # BERTScore\n    P, R, F1 = bert_score(preds, labels, lang=\"fa\", rescale_with_baseline=False)  # Specify the language if necessary\n    avg_bert_f1 = F1.mean().item()\n\n    return {\n        'bleu': avg_bleu, \n        'meteor': avg_meteor, \n        'rouge1': rouge_scores['rouge1'], \n        'rouge2': rouge_scores['rouge2'], \n        'rougeL': rouge_scores['rougeL'],\n        'bertscore_f1': avg_bert_f1\n    }\n\ndef evaluate_model(model, tokenizer, data_loader):\n    model.eval()\n    predictions = []\n    references = []\n    vocab_size = tokenizer.vocab_size\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(model.device)\n            attention_mask = batch['attention_mask'].to(model.device)\n\n            if 'labels' in batch:\n                labels = batch['labels'].to(model.device)\n\n            # Generate predictions\n            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n\n            # Ensure generated output tokens are within the valid range of the model's vocabulary\n            outputs = outputs.clamp(0, vocab_size - 1)\n            \n            # Decode generated tokens\n            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n            if 'labels' in batch:\n                # Ensure reference labels are clamped before decoding\n                labels = labels.clamp(0, vocab_size - 1)\n                refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n            else:\n                refs = []\n\n            predictions.extend(preds)\n            references.extend(refs)\n    \n    if len(predictions) == 0:\n        print('No predictions generated.')\n        return {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0, \"bleu\": 0, \"meteor\": 0}\n\n    # Compute metrics\n    metrics = compute_metrics(predictions, references)\n    return metrics\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:39:17.934741Z","iopub.execute_input":"2024-10-16T12:39:17.935676Z","iopub.status.idle":"2024-10-16T12:39:20.364157Z","shell.execute_reply.started":"2024-10-16T12:39:17.935635Z","shell.execute_reply":"2024-10-16T12:39:20.363407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\").to(device)\nfinetuned_dir = f\"./results/checkpoint\"\n# Test the fine-tuned model (assuming the best fine-tuned model is saved in './results/best_checkpoint')\nfine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(finetuned_dir).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:39:26.423691Z","iopub.execute_input":"2024-10-16T12:39:26.424543Z","iopub.status.idle":"2024-10-16T12:39:37.167740Z","shell.execute_reply.started":"2024-10-16T12:39:26.424505Z","shell.execute_reply":"2024-10-16T12:39:37.166236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nimport transformers\nimport torch\n\n# Set the logging level to WARNING or ERROR to suppress info/debug logs\nlogging.getLogger('transformers').setLevel(logging.ERROR)  # Suppress transformer messages\nlogging.getLogger('torch').setLevel(logging.ERROR)\n\nprint(\"Evaluating pre-trained mT5...\")\npretrained_metrics = evaluate_model(pretrained_model, tokenizer, test_loader)\n\nprint(\"Evaluating fine-tuned mT5...\")\nfine_tuned_metrics = evaluate_model(fine_tuned_model, tokenizer, test_loader)\n\n# Create a table with an extra column for BERTScore F1\ntable = PrettyTable()\ntable.field_names = [\"Model\", \"BLEU\", \"METEOR\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERTScore F1\"]\n\n# Add pre-trained mT5 results\ntable.add_row([\n    \"Pre-trained mT5\",\n    f\"{pretrained_metrics['bleu']:.4f}\",\n    f\"{pretrained_metrics['meteor']:.4f}\",\n    f\"{pretrained_metrics['rouge1']:.4f}\",\n    f\"{pretrained_metrics['rouge2']:.4f}\",\n    f\"{pretrained_metrics['rougeL']:.4f}\",\n    f\"{pretrained_metrics['bertscore_f1']:.4f}\"  # Adding BERTScore F1\n])\n\n# Add fine-tuned mT5 results\ntable.add_row([\n    \"Fine-tuned mT5\",\n    f\"{fine_tuned_metrics['bleu']:.4f}\",\n    f\"{fine_tuned_metrics['meteor']:.4f}\",\n    f\"{fine_tuned_metrics['rouge1']:.4f}\",\n    f\"{fine_tuned_metrics['rouge2']:.4f}\",\n    f\"{fine_tuned_metrics['rougeL']:.4f}\",\n    f\"{fine_tuned_metrics['bertscore_f1']:.4f}\"  # Adding BERTScore F1\n])\n\n# Print the table to the console\nprint(table)\n\n# Save the metrics to a log file\noutput_file = \"model_metrics_log.txt\"\nwith open(output_file, 'w') as f:\n    f.write(\"Model Evaluation Metrics:\\n\")\n    f.write(str(table))\n\nprint(f\"Metrics saved to {output_file}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T12:39:42.483052Z","iopub.execute_input":"2024-10-16T12:39:42.484063Z","iopub.status.idle":"2024-10-16T12:39:58.135194Z","shell.execute_reply.started":"2024-10-16T12:39:42.484018Z","shell.execute_reply":"2024-10-16T12:39:58.134246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_test_outputs_to_file(model, tokenizer, test_loader, output_file):\n    model.eval()\n\n    # Open file to save results\n    with open(output_file, 'w') as f:\n        # Loop through the test data\n        for batch in test_loader:\n            inputs = batch['input_ids'].to(model.device)\n            attention_mask = batch['attention_mask'].to(model.device)\n\n            # Generate predictions from the model\n            with torch.no_grad():\n                generated_ids = model.generate(\n                    input_ids=inputs,\n                    attention_mask=attention_mask,\n                    max_length=128, \n                    num_beams=4,   \n                    early_stopping=True\n                )\n\n            # Decode the input and generated output tokens\n            for i, input_id in enumerate(inputs):\n                input_text = tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n                # Ensure generated_ids are within valid range\n                valid_generated_ids = torch.clamp(generated_ids[i], min=0, max=tokenizer.vocab_size - 1).tolist()\n                generated_text = tokenizer.decode(valid_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n                generated_text = generated_text.replace('<extra_id_0>', '').replace('<extra_id_1>', '') \n\n                # If reference output is available (for evaluation purposes)\n                if 'labels' in batch:\n                    reference_ids = batch['labels'][i]\n\n                    # Remove padding token IDs before decoding\n                    reference_ids = reference_ids[reference_ids != tokenizer.pad_token_id]\n\n                    # Convert to a list and ensure reference_ids are within valid range\n                    reference_ids_list = torch.clamp(reference_ids, min=0, max=tokenizer.vocab_size - 1).tolist()\n\n                    # Decode the reference text\n                    reference_text = tokenizer.decode(reference_ids_list, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                else:\n                    reference_text = \"N/A\"\n\n                # Write the input, generated output, and reference output to the file\n                f.write(f\"Input: {input_text}\\n\")\n                f.write(f\"Generated Output: {generated_text}\\n\")\n                f.write(f\"Reference Output: {reference_text}\\n\")\n                f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")  \n\n    print(f\"Test outputs saved to {output_file}\")\n\nf_output_file = \"finetuned_model_test_outputs.txt\"\nsave_test_outputs_to_file(fine_tuned_model, tokenizer, test_loader, f_output_file)\n\np_output_file = \"pretrained_model_test_outputs.txt\"\nsave_test_outputs_to_file(pretrained_model, tokenizer, test_loader, p_output_file)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T13:43:08.673586Z","iopub.execute_input":"2024-10-14T13:43:08.674472Z","iopub.status.idle":"2024-10-14T13:43:11.781719Z","shell.execute_reply.started":"2024-10-14T13:43:08.674428Z","shell.execute_reply":"2024-10-14T13:43:11.780781Z"},"trusted":true},"outputs":[],"execution_count":null}]}